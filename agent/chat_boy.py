#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡ä»¶åˆ†æåŠ©æ‰‹ï¼ˆä¿®æ­£ç‰ˆï¼‰
æ ¹æ® info_chatboy_ver1.md çš„å»ºè®®é‡æ„ï¼š
- thinking åªå‘ç”Ÿä¸€æ¬¡ï¼Œä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
- decide æ˜¯çº¯ç¨‹åºé€»è¾‘ï¼Œä¸è°ƒç”¨ LLM
- act åªæ‰§è¡Œå·¥å…·ï¼Œä¸è°ƒç”¨ LLM
- observe åæ¡ä»¶åˆ¤æ–­ï¼Œä¸æ— æ¡ä»¶é‡æ–°æ€è€ƒ
"""

import os
import operator
import json
from typing import Annotated, Literal, Optional
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage, AnyMessage
from langchain.tools import tool
from pydantic import SecretStr, BaseModel, Field
from langgraph.checkpoint.memory import MemorySaver

from load_env import API_KEY, BASE_URL

# åˆå§‹åŒ– LLMï¼ˆéæµå¼è¾“å‡ºï¼‰
llm = ChatOpenAI(
    streaming=False,
    api_key=SecretStr(API_KEY),
    base_url=BASE_URL,
    model="gpt-5.1",
    temperature=0
)


# å®šä¹‰å·¥å…·
@tool
def list_directory(directory_path: str = ".") -> str:
    """list file under specific directory
    
    Args:
        directory_path: dir path to be listed, default is current dir: '.'
    
    Returns:
        content list of a directory
    """
    try:
        if not os.path.isabs(directory_path):
            directory_path = os.path.abspath(directory_path)
        
        if not os.path.exists(directory_path):
            return f"Error: dir '{directory_path}' doesn't exist"
        
        if not os.path.isdir(directory_path):
            return f"Error:'{directory_path}' is not a directory"
        
        items = []
        for item in sorted(os.listdir(directory_path)):
            item_path = os.path.join(directory_path, item)
            if os.path.isdir(item_path):
                items.append(f"[dir] {item}/")
            else:
                size = os.path.getsize(item_path)
                items.append(f"[file] {item} ({size} bytes)")
        
        result = f"content of dir '{directory_path}':\n" + "\n".join(items)
        return result
    except Exception as e:
        return f"Error: failed to list directory - {str(e)}"


@tool
def read_file_tool(file_path: str) -> str:
    """read content of certain file
    
    Args:
        file_path: path of target file (absolute or relative path)
    
    Returns:
        file content. if doesn't exist, it is error message.
    """
    try:
        if not os.path.isabs(file_path):
            file_path = os.path.abspath(file_path)
        
        if not os.path.exists(file_path):
            return f"Error: file '{file_path}' doesn't exist"
        
        if not os.path.isfile(file_path):
            return f"Error: file '{file_path}' is not a file"
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return f"file: '{file_path}' , length: {len(content)}, content:\n\n{content}"
        except UnicodeDecodeError:
            with open(file_path, 'rb') as f:
                content = f.read()
            return f"file '{file_path}' is a binary file with length {len(content)}, cannot displayed as text."
    except Exception as e:
        return f"Error in reading file - {str(e)}"


# ç»‘å®šå·¥å…·åˆ° LLM
tools = [list_directory, read_file_tool]
tools_by_name = {tool.name: tool for tool in tools}
model_with_tools = llm.bind_tools(tools)


# å®šä¹‰ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹
class ThinkingOutput(BaseModel):
    """æ€è€ƒè¾“å‡ºç»“æ„"""
    thinking: str = Field(description="Your thinking process about the user's request")
    plan: str = Field(description="Your step-by-step plan to solve the problem")
    next_action: Literal["use_tool", "ask_user", "respond"] = Field(
        description="What to do next: use_tool if need to call a tool, ask_user if need more info, respond if can answer now"
    )
    tool_name: Optional[str] = Field(
        default=None,
        description="If next_action is 'use_tool', specify which tool: 'list_directory' or 'read_file_tool'"
    )
    tool_args: Optional[dict] = Field(
        default=None,
        description="If next_action is 'use_tool', specify the tool arguments"
    )
    question: Optional[str] = Field(
        default=None,
        description="If next_action is 'ask_user', specify the question to ask"
    )


# ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºçš„ LLM
structured_llm = llm.with_structured_output(ThinkingOutput)


class State(TypedDict):
    """Agent çŠ¶æ€"""
    messages: Annotated[list[AnyMessage], operator.add]
    thinking: str  # æ€è€ƒè¿‡ç¨‹ï¼ˆç”¨äºæ˜¾ç¤ºï¼Œä¸æ³¨å…¥åˆ° promptï¼‰
    plan: str  # è®¡åˆ’ï¼ˆç”¨äºæ˜¾ç¤ºï¼Œä¸æ³¨å…¥åˆ° promptï¼‰
    next_action: Optional[str]  # ä¸‹ä¸€æ­¥è¡ŒåŠ¨
    tool_name: Optional[str]  # è¦è°ƒç”¨çš„å·¥å…·å
    tool_args: Optional[dict]  # å·¥å…·å‚æ•°
    question: Optional[str]  # è¦é—®ç”¨æˆ·çš„é—®é¢˜
    task_complete: bool  # ä»»åŠ¡æ˜¯å¦å®Œæˆ


def thinking_node(state: State):
    """æ€è€ƒèŠ‚ç‚¹ï¼šä¸€æ¬¡æ€§å®Œæˆæ€è€ƒã€è®¡åˆ’ã€å†³ç­–ï¼ˆåªè°ƒç”¨ä¸€æ¬¡ LLMï¼‰"""
    system_prompt = """You are a professional file-analyzing assistant. 

You have access to these tools:
- list_directory: list the content of a directory
- read_file_tool: read the content of a file

Analyze the user's request, think about what you need, make a plan, and decide what to do next.

Think step by step, then output your plan and next action."""
    
    # è·å–æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆåªå–æœ€åå‡ æ¡æ¶ˆæ¯ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ï¼‰
    recent_messages = state["messages"][-10:]  # åªå–æœ€è¿‘10æ¡æ¶ˆæ¯
    
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = [SystemMessage(content=system_prompt)]
    messages.extend(recent_messages)

    # print(messages)
    
    print("\n" + "="*60)
    print("ğŸ¤” THINKING:")
    print("="*60)
    
    # è°ƒç”¨ç»“æ„åŒ– LLMï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼‰
    try:
        print(messages)
        result = structured_llm.invoke('hello, please think')
        print('*****************')
        result = structured_llm.invoke(messages)
        # result = llm.invoke(messages)
        # print(result)
        
        thinking_content = result.thinking
        plan_content = result.plan
        next_action = result.next_action
        
        print(thinking_content)
        print("\n" + "-"*60)
        print("ğŸ“‹ PLAN:")
        print("-"*60)
        print(plan_content)
        print("="*60 + "\n")
        
        # è¿”å›ç»“æ„åŒ–ç»“æœ
        return {
            "thinking": thinking_content,
            "plan": plan_content,
            "next_action": next_action,
            "tool_name": result.tool_name,
            "tool_args": result.tool_args,
            "question": result.question,
            "task_complete": False
        }
    except Exception as e:
        print(f"Error in thinking: {e}")
        # å¦‚æœç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ° respond
        return {
            "thinking": f"Error in thinking: {e}",
            "plan": "Unable to create plan",
            "next_action": "respond",
            "task_complete": False
        }


def decide_node(state: State):
    """å†³ç­–èŠ‚ç‚¹ï¼šçº¯ç¨‹åºé€»è¾‘ï¼Œä¸è°ƒç”¨ LLMï¼ˆåªç”¨äºæ˜¾ç¤ºï¼‰"""
    next_action = state.get("next_action", "respond")
    print(f"\nğŸ“‹ DECISION: {next_action.upper()}\n")
    return {}  # ä¸ä¿®æ”¹çŠ¶æ€ï¼Œåªç”¨äºæ˜¾ç¤º


def route_after_decide(state: State) -> Literal["act", "ask_user", "respond", END]:
    """è·¯ç”±å‡½æ•°ï¼šæ ¹æ®å†³ç­–ç»“æœè·¯ç”±"""
    next_action = state.get("next_action", "respond")
    
    if next_action == "use_tool":
        return "act"
    elif next_action == "ask_user":
        return "ask_user"
    elif next_action == "respond":
        return "respond"
    else:
        return END


def act_node(state: State):
    """æ‰§è¡ŒèŠ‚ç‚¹ï¼šåªæ‰§è¡Œå·¥å…·ï¼Œä¸è°ƒç”¨ LLM"""
    tool_name = state.get("tool_name")
    tool_args = state.get("tool_args", {})
    
    if not tool_name or tool_name not in tools_by_name:
        print(f"âŒ Error: Invalid tool name '{tool_name}'")
        return {
            "messages": [AIMessage(content=f"Error: Invalid tool name '{tool_name}'")],
            "task_complete": True
        }
    
    tool = tools_by_name[tool_name]
    
    print(f"\nğŸ”§ ACTING: Calling {tool_name}")
    print(f"   Args: {tool_args}\n")
    
    try:
        # åªæ‰§è¡Œå·¥å…·ï¼Œä¸è°ƒç”¨ LLM
        observation = tool(**tool_args)
        print(f"   âœ“ Tool executed successfully\n")
        
        # åˆ›å»ºå·¥å…·æ¶ˆæ¯ï¼ˆæ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ¨¡æ‹Ÿ tool_call_idï¼Œå®é™…åº”è¯¥ä» thinking é˜¶æ®µè·å–ï¼‰
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥åˆ›å»º ToolMessage
        tool_message = ToolMessage(
            content=str(observation),
            tool_call_id=f"call_{tool_name}_{hash(str(tool_args))}"
        )
        
        return {"messages": [tool_message]}
    except Exception as e:
        error_msg = f"Error executing tool {tool_name}: {str(e)}"
        print(f"   âœ— {error_msg}\n")
        return {
            "messages": [ToolMessage(content=error_msg, tool_call_id="error")],
            "task_complete": True
        }


def observe_node(state: State):
    """è§‚å¯ŸèŠ‚ç‚¹ï¼šå¤„ç†å·¥å…·æ‰§è¡Œç»“æœ"""
    last_message = state["messages"][-1] if state["messages"] else None
    
    if isinstance(last_message, ToolMessage):
        print("\nğŸ‘€ OBSERVING: Tool result received\n")
        # å·¥å…·ç»“æœå·²æ”¶åˆ°ï¼Œç»§ç»­æµç¨‹
        return {}
    
    return {}


def should_continue_after_observe(state: State) -> Literal["thinking", "respond", END]:
    """è§‚å¯Ÿååˆ¤æ–­ï¼šæ˜¯å¦éœ€è¦é‡æ–°æ€è€ƒæˆ–ç›´æ¥å›å¤"""
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å·¥å…·ç»“æœæ¥å›ç­”é—®é¢˜
    tool_messages = [msg for msg in state["messages"] if isinstance(msg, ToolMessage)]
    user_messages = [msg for msg in state["messages"] if isinstance(msg, HumanMessage)]
    
    # å¦‚æœæœ‰å·¥å…·ç»“æœï¼Œæ£€æŸ¥æ˜¯å¦è¿˜éœ€è¦æ›´å¤šä¿¡æ¯
    if tool_messages:
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å¯èƒ½å®Œæˆï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ›´æ™ºèƒ½ï¼‰
        # å¦‚æœå·¥å…·è°ƒç”¨æˆåŠŸï¼Œé€šå¸¸å¯ä»¥å°è¯•å›ç­”
        last_tool_msg = tool_messages[-1]
        if "Error" in last_tool_msg.content:
            # å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œå¯èƒ½éœ€è¦é‡æ–°æ€è€ƒæˆ–è¯¢é—®ç”¨æˆ·
            return "thinking"
        else:
            # å·¥å…·æ‰§è¡ŒæˆåŠŸï¼Œå¯ä»¥å°è¯•å›ç­”
            return "respond"
    
    # æ²¡æœ‰å·¥å…·ç»“æœï¼Œç›´æ¥å›å¤
    return "respond"


def respond_node(state: State):
    """å“åº”èŠ‚ç‚¹ï¼šç”Ÿæˆæœ€ç»ˆå›å¤"""
    system_prompt = """You are a professional file-analyzing assistant. Based on the conversation history and any tool results, provide a clear and helpful answer to the user.

Do NOT repeat your thinking process or plan in the response. Just provide the answer directly."""
    
    messages = [SystemMessage(content=system_prompt)]
    messages.extend(state["messages"])
    
    # æ³¨æ„ï¼šä¸æ³¨å…¥ thinking/plan æ–‡æœ¬åˆ° promptï¼ˆé¿å…åæ¨¡å¼ï¼‰
    # thinking/plan åªç”¨äºæ˜¾ç¤ºå’Œç¨‹åºé€»è¾‘
    
    print("\nğŸ’¬ RESPONDING: Generating answer...\n")
    
    response = llm.invoke(messages)
    
    return {
        "messages": [response],
        "task_complete": True
    }


def ask_user_node(state: State):
    """è¯¢é—®ç”¨æˆ·èŠ‚ç‚¹ï¼šå‘ç”¨æˆ·è¯·æ±‚æ›´å¤šä¿¡æ¯"""
    question = state.get("question", "I need more information to help you. Could you please provide more details?")
    
    print("\nâ“ ASKING USER:\n")
    print(question)
    print()
    
    # åˆ›å»ºè¯¢é—®æ¶ˆæ¯
    ask_message = AIMessage(content=question)
    
    return {
        "messages": [ask_message],
        "task_complete": False
    }


def build_graph():
    """æ„å»º Agent å›¾"""
    graph_builder = StateGraph(State)
    
    # æ·»åŠ èŠ‚ç‚¹
    graph_builder.add_node("thinking", thinking_node)
    graph_builder.add_node("decide", decide_node)
    graph_builder.add_node("act", act_node)
    graph_builder.add_node("observe", observe_node)
    graph_builder.add_node("respond", respond_node)
    graph_builder.add_node("ask_user", ask_user_node)
    
    # è®¾ç½®å…¥å£
    graph_builder.add_edge(START, "thinking")
    
    # æ€è€ƒåè¿›å…¥å†³ç­–ï¼ˆçº¯ç¨‹åºé€»è¾‘ï¼‰
    graph_builder.add_edge("thinking", "decide")
    
    # å†³ç­–åæ ¹æ®ç»“æœè·¯ç”±
    graph_builder.add_conditional_edges(
        "decide",
        route_after_decide,  # ä½¿ç”¨è·¯ç”±å‡½æ•°
        {
            "act": "act",
            "ask_user": "ask_user",
            "respond": "respond",
            END: END
        }
    )
    
    # æ‰§è¡Œå·¥å…·åè§‚å¯Ÿç»“æœ
    graph_builder.add_edge("act", "observe")
    
    # è§‚å¯Ÿåæ¡ä»¶åˆ¤æ–­
    graph_builder.add_conditional_edges(
        "observe",
        should_continue_after_observe,
        {
            "thinking": "thinking",  # éœ€è¦é‡æ–°æ€è€ƒ
            "respond": "respond",  # å¯ä»¥ç›´æ¥å›å¤
            END: END
        }
    )
    
    # å“åº”å’Œè¯¢é—®ç”¨æˆ·åç»“æŸ
    graph_builder.add_edge("respond", END)
    graph_builder.add_edge("ask_user", END)
    
    # ä½¿ç”¨ checkpoint æ”¯æŒè®°å¿†
    checkpointer = MemorySaver()
    
    return graph_builder.compile(checkpointer=checkpointer)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¤– Smart File Helper (Corrected Version)")
    print("=" * 60)
    print(f"\nCurrent working directory: {os.getcwd()}")
    print("You can ask me to analyze files, list directories, etc.\n")
    print("Type 'quit' or 'exit' to quit.\n")
    
    # æ„å»ºå›¾
    graph = build_graph()
    
    # ä½¿ç”¨ checkpoint ç®¡ç†å¯¹è¯çŠ¶æ€
    thread_id = "chat-boy-session"
    config = {"configurable": {"thread_id": thread_id}}
    
    # äº¤äº’å¼å¾ªç¯
    while True:
        try:
            user_input = input("User> ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ["quit", "exit", "q"]:
                print("\nBye! ğŸ‘‹\n")
                break
            
            # å‡†å¤‡è¾“å…¥ï¼ˆåªä¼ å…¥æ–°æ¶ˆæ¯ï¼Œcheckpoint ä¼šè‡ªåŠ¨ç»´æŠ¤å†å²çŠ¶æ€ï¼‰
            inputs = {
                "messages": [HumanMessage(content=user_input)],
                "thinking": "",
                "plan": "",
                "next_action": None,
                "tool_name": None,
                "tool_args": None,
                "question": None,
                "task_complete": False
            }
            
            # æ‰§è¡Œå›¾ï¼ˆcheckpoint ä¼šè‡ªåŠ¨ä¿å­˜å’Œæ¢å¤çŠ¶æ€ï¼‰
            result = graph.invoke(inputs, config=config)
            
            # æ˜¾ç¤ºæœ€ç»ˆå›å¤
            if result.get("messages"):
                last_message = result["messages"][-1]
                if isinstance(last_message, AIMessage):
                    print("\n" + "="*60)
                    print("ğŸ¤– ASSISTANT:")
                    print("="*60)
                    print(last_message.content)
                    print("="*60 + "\n")
            
        except KeyboardInterrupt:
            print("\n\nBye! ğŸ‘‹\n")
            break
        except Exception as e:
            print(f"\nâŒ Error: {e}\n")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
